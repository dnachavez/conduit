I'm planning to build a proof-of-concept AI call center agent using OpenAI and Amazon Web Services (AWS). The stack that I'm planning to use is Next.js and TypeScript for the frontend, Supabase for the database, Python for the backend and agents, Twilio for the gateway, and LiveKit. I need to complete the proof-of-concept or at least have it at 90% within a day. In this platform a representative of a business or company can create an account using their business or company email address and setup an initial workspace. Each workspace will have its own agent, phone number, and knowledge base. The representative can setup the knowledge base by adding a link to their company's help center for the platform to scrape, write knowledge base articles, and upload documents like Word or PDF documents. Multiple workspaces can be created by the business or company representative as the idea is for each department for example to have their own workspace, knowledge base, and agent that's separate from other workspaces. During calls, the platform should be able to record a transcript of the conversation between agent-customer or customer-human agent in real-time. If the agent can no longer respond to the customer, it should hand off the call to an available human agent passing all contexts that it has on the call such as intent, sentiment, topic, knowledge used, and transcript. In this way a human agent can have all the information it needs or might need to help resolve the customer's call. The idea is for Twilio to become the gateway from phone network into LiveKit. Once all the call is in LiveKit, the agent and browser humans are all just participating in a room. It will start with a customer dialing the workspace's Twilio number. Twilio then forwards the call to LiveKit over SIP using LiveKit's Twilio integration ("inbound trunk") and each call lands in its own LiveKit room. The agent then joins the LiveKit room where is our agent listens to the audio stream from the caller, uses an STT backend + OpenAI + TTS or OpenAI Voice, and speak back into the room with synthesized audio. This will give us a natural, interruptible, low-latency AI voice agent that can use the knowledge base of the workspace to answer. If the agent can no longer resolve the customer's call I want the system to pick an idle human agent and show the a join call button. Then the human agent joins the same LiveKit room via WebRTC and can talk to the caller directly. And because everyone's in the same LiveKit room, the switch is seamless and there's no extra Twilio bridging and we can always make sure that the agent can give support or help to human agents by suggesting what to say next, how to resolve the issue based on the knowledge base, and etc.